Скрипт предназначен для получения доступных данных с сайта:
[https://www.marinetraffic.com/](https://www.marinetraffic.com/)

![Alt process](https://drive.google.com/uc?export=view&id=1aJwG_U28rOXkzyyvSRZ_W2Xgmo8OYdBj)

# Запуск

Выполнить для загрузки пакетов

```shell
npm install
```

Запустить скрипт

```shell
node ship-scraper.js [--ключ=значение [,--ключ=значение, ...]]
```

# Описание

Выбран способ получения данных, путём перехвата запросов на JSON данные, по которым шаблонируется страница.
Разбор страницы по тегам нереализован т.к. привязка по селекторам ужасна (неименованные селекторы, очень длинная цепочка - сраница шаблонизирована).

# Опции

- [--search[ = "" ]] - имя судна, MMSI номер, или имя файла со списком (напр. "SARA", "511100217", "ship-list.txt");
- [--list[ = false ]] - ключ, используется в случае если в --search файл со списком;
- [--type[ = 0 ]] - тип поиска 0 - все элементы, 1- по имени судна (уменьшает количество, найденного);
- [--page[ = 50 ]] - количество записей на одном поисковом листе;
- [--limit[ = 0 ]] - ограничение на количество страниц, в результате количество судов ≤ page \* limit;
- [--hidden[ = true ]] - скрыть окно браузера;
- [--strict[ = false ]] - строгий режим, имя найденного коробля 100% совпадение запрашиваемому, работает только с именем судна;
- [--dateformat[ = "dd.mm.yyyy ]] - формат времени в csv, подробнее [dateformat](https://www.npmjs.com/package/dateformat)
- [--timeout[ = 30000 ]] - Задержка на странице, в случае если данные не найдены. Ниже 5000 мс - ошибка. Необходима в случае медленной сети.

# Результат

В результате выполнения скрипта, появится два файла:

1. [имя судна].csv - содержащий по возможности таблицу в формате: имя суда порт; отправления; дата отправления;
2. [имя судна].json - содержит данные JSON список [voyageInfo, vesselInfo, latestPosition], соответственно если данные получены.
3. [имя списка].rez - содержит общий список кораблей, для случае когда задан список

# Отмазка

Написано криво :)
